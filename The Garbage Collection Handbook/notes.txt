Mark-Sweep (Ch. 2)
- mark traverses from roots and sets mark bit on live objects
- sweep traverses heap and reclaims unmarked objects
- performance is a huge concern for GCs
	- bitmaps can be used (rather than bits in each object header) to improve cache performance
	- techniques to minimize recursion stack
- lazy sweeping amortizes O(heap size) sweep across allocations
	- collection marks all blocks, creating free list (as usual) and "reclaim list" of candidate blocks for sweeping
	- allocation exhausts free list, then incrementally sweeps reclaim list only until block found
- mark has poor locality since it traverses a pointer graph; memory prefetching useful here
	- since mark is a DFS, hard to prefetch (when will an object be popped? the prefetched memory could be evicted)
	- prefetching and enqueueing objects popped off the stack into a fixed-size queue before marking means that, when dequeued, they should be in cache
	- checking the mark bits of children before pushing onto mark stack still leads to cache misses
		- pushing all children indiscriminately leads to repeated iterations (O(# edges) vs. O(# nodes)), but avoids cache misses

Mark-Compact (Ch. 3)
- mark-sweep is good, but doesn't address fragmentation
- compact phase "compacts" live blocks, moving them into contiguous heap memory
	- allocation is just bumping a free pointer
	- compaction order typically sliding (order preserved), but could be arbitrary (harms locality) or linearizing (pointees close to pointers)
- two-finger compaction (arbitrary, 2 pass) scans from heap start and end, moving objects from end into open spots at start
	- first pass moves, leaving forwarding pointer at old spot
	- second pass updates references pointing to old location
- lisp 2 (sliding, 3 pass) uses dedicated forwarding address field inside header
	- first pass computes new locations
	- second pass updates references pointing to old location
	- third pass moves everything to new locations
- Jonkers (sliding, 2 pass) avoids the extra forwarding address field, using pointer "threading"
	- by reversing direction of pointers to each object, can construct a linked list of fields which point to each object
	- first pass (forward references) threads all pointer fields and updates forward references to X to point to where it will end up
		- inductively, all forward references to X will already be threaded, so we just traverse list and update each reference
	- second pass (backward references) updates backward references to X to point to where it will end up, and then moves X
- Compressor (sliding, 1 pass) and similar 1-pass algorithms leverage side-tables (metadata is not stored in object headers)
	- mark bits stored in a bit vector
	- construct a mapping for the new starting offset for each block of memory (rather than each object)
	- for each object, use bit vector to compute an offset to add to the starting offset

Copying (Ch. 4)
- compacted heaps improve speed and locality of allocations, but collection can be slow
- split heap into 2 semispaces, from-space and to-space, and copy live objects from former to latter during collection
- different traversal orders when copying affect locality
	- Cheney's algorithm uses the to-space as its work list, performing a BFS, which separates parents from children
	- DFS is better than BFS, but incurs space overhead
	- Moon's modification to Cheney's approximates DFS: secondary scan performed over the last to-space page not completely scanned before continuing primary scan
	- optimal arrangement is NP-complete. can also profile field accesses and copy "hot" fields over with objects

Reference Counting (Ch. 5)
- counter of references to object maintained in header; object freed when count reaches 0
- memory (usually) freed as soon as it is unreachable, unlike tracing
- however, counting slows mutator code, requires atomicity, increases memory overhead and usage, and cannot reclaim cyclic structures
- deferred reference counting reduces overhead by only updating counts based on heap references
	- many reference count changes caused by locals (and other roots); this avoids many atomic increment and decrement operations
	- stop-the-world collection required; increments counts based on root references before determining garbage
- coalesced reference counting avoids redundant increment + decrement operations
	- e.g. in X <- obj1, X <- obj2, .. X <- objn, we need only decrement obj0 and increment objn
	- logs old values of an object's pointer fields; at end of epoch, increments for current values and decrements for logged values
- Recycler detects cycles by checking candidates (objects whose reference counts, after a decrement, are nonzero) during an atomic collect phase
	- speculatively "deletes" candidate recursively; if it results in a zero reference count, object deleted, otherwise counts restored
- the count field in an object header can theoretically be pointer-sized (i.e. object referenced by every other object), which is a waste
	- can treat objects whose counts exceed a maximum differently (e.g. with a tracing collector)

Comparing Garbage Collectors (Ch. 6)
- the right collector depends on application requirements
- throughput (overall running time) 
	- minimizing collector time not necessarily optimal if it increases mutator time explicitly (e.g. read/write barriers) or implicitly (e.g. bad cache interactions)
- pause time (time the world is stopped for collection)
	- system could have real-time constraints or be detrimentally affected by pauses (e.g. workload buildup)
- space (memory usage)
	- collectors use memory (reference count fields, free lists, semispaces, etc.)
	- some collectors free memory immediately as it becomes garbage, while others take longer to free it
- implementation (of the collection algorithm and more importantly its runtime interface)
	- different algorithms impose different requirements on application code (e.g. reference counting requires read/write barriers)
- research exists for dynamically adapting a collection strategy, statically determining a strategy using ML, and choosing a strategy based on user-provided constraints
- abstractly, garbage collection is a fixed-point computation to assign reference counts to nodes
	- under this framework, tracing and reference counting are very similar
	- tracing computes a least fixed point whereas reference counting computes a greatest fixed point 
	- the difference is cyclic garbage, which reference counting cannot detect (hence why tracing is used as a fallback)

Allocation (Ch. 7)
- sequential allocation (bumping a free pointer) is simple and efficient, but doesn't work as well with non-moving collectors
- free-list allocation (data structure maintaining list of free cells) more complex, but gives more control
    - sequential search strategy classically one of 3:
        - first-fit (start at beginning and return first cell that fits)
        - next-fit (first-fit, but start from last allocation)
        - best-fit (search entire list for best fit)
    - can improve on sequential search using balanced BSTs or bitmaps
- external fragmentation (free memory dispersed into many small gaps) can cause allocation failures or wasted memory space
    - unavoidable, but different allocation strategies affect its extent
- segregated-fits allocation designates different size classes which are allocated separately; this mitigates fragmentation
    - for all allocations exceeding the maximum size class, some other strategy still needs to be used
    - introduces internal fragmentation, where an object is allocated more space than it needs because of the size class constraint
    - a couple of approaches are common for populating free lists for each size class:
        - big bag of pages: when a block with some size is allocated for use by a cell of some other size, the whole block is used for cells of that size (and possibly type)
        - given a cell of size 2^(i+1), can split into two cells of size 2^i (repeatedly); the same 2 cells can be coalesced back into the original-size cell
- there are many considerations allocators need to make, including:
    - alignment (some objects may need to be aligned on certain boundaries, e.g. java doubles are 8-byte aligned)
    - size constraints (some collection schemes require space for metadata)
    - heap parsability (ability to traverse the heap and understand what objects exist in it)
    - locality (both for allocation and for freeing)
- in concurrent systems, allocation can be a significant bottleneck because of atomicity requirements
    - often, each thread (or processor) is given its own local allocator, which avoids the need for synchronization

Partitioning the heap (Ch. 8)
- the heap can be partitioned into spaces managed with different policies/by different mechanisms
- there are many different properties/metrics one could partition on/for, such as:
    - mobility (e.g. copying/compacting collection cannot be used on immobile objects)
    - size (e.g. large objects are expensive to move)
    - kind (e.g. a partition with pointer-free objects need not be scanned)
    - yield (e.g. collecting young objects reclaims more memory than old objects)
    - thread (e.g. thread-local objects can be collected without synchronization)
    - mutability (e.g. frequently modified objects incur a high overhead with reference counting)
- spaces can be partitioned as contiguous areas (sometimes inefficient; large address space reserved), discontiguous chunks (requires table lookup), or not partitioned at all (using header bits to indicate an object's space)
- partitioning can be done by:
    - the collector (e.g. generational collectors when objects are promoted, or copying collectors for immovable objects)
    - the allocator (e.g. large allocations put in separate partition, thread-local objects put in local partition)
    - the compiler (e.g. an object is statically known to live for a long time)
    - the mutator (e.g. read/write barriers might move/mark objects)

Generational Collection (Ch. 9)
- motivated by the observation that most objects die young
    - for objects which don't die young, no clear correlation in object lifetime
- separates heap objects based on age and prioritizes collecting younger generations
    - faster since time isn't wasted scanning older generations
    - more memory reclaimed since old objects less frequently become garbage
- "age" measured in terms of collections survived (memory allocated since creation an uncommon alternative)
- size of younger generation is a balancing act
    - too large, and young collections will take a long time
    - too small, and more collections will occur, and consequently more young objects get promoted before they can die
        - "nepotism" happens when promoted objects become garbage but keep other young garbage alive (leading to their promotion)
        - a write barrier is required to detect cross-generational pointers; since new objects modified more than old objects, increases mutator overhead
        - the old generation will fill up quicker, leading to more frequent full collections
- using multiple (>2) generations enables smaller young generation without causing as many full collections
    - when collecting any generation, all younger generations also collected; thus only pointers from old to new generations need be tracked (uncommon)
    - more complex to implement and could increase overhead of write barrier
- instead of more generations, one can use various mechanisms to control how objects are promoted:
    - en masse, where all live objects in a generation are promoted (simple, but requiring multiple collections before promotion tends to reduce garbage)
    - aging semispaces, where entire semispace is promoted after some number of copies (younger objects indiscriminately promoted)
        - collect at object granularity by recording age in header
        - "bucket brigade" separates generation into n semispaces, requiring data to reach nth semispace before promotion
        - semispaces can use a lot of memory; can use an "eden" region for new allocations and 2 "survivor" semispaces for object surviving eden collection
- since workloads can vary, some collectors adapt to program behaviour, e.g. by maximizing nursery space under current constraints, or using feedback
- the "roots" for a generation include pointers into it from other generations
    - remembered sets can be maintained which track objects potentially containing inter-generational pointers (requires write barrier)
    - if we always collect the generations younger than the one we want to collect, we only need to track pointers from older to newer generations
- younger generations almost always distinguished by physical location, so younger generations generally managed using copying
- old objects infrequently change, and copying is wasteful, so the oldest generation is usually managed using mark-sweep (and maybe infrequent compaction)
- some applications suffer because of objects that die shortly after they are tenured
    - older-first collection algorithms give younger objects more time to die without wasting too much time on long-lived objects
- generational collections do well with short-lived objects, but longer-lived ones suffer, since their collection and promotion are expensive
    - researchers have used profiling, machine learning over a corpus of existing program traces, and static analyses to pre-tenure objects
